{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\n\nimport scipy as sp\nimport pandas as pd\nimport re\nimport numpy as np\n\n\n# Data visualization\nimport matplotlib as mlt\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Scalers\nfrom sklearn.preprocessing import StandardScaler\n\n#discritization\nfrom sklearn.preprocessing import LabelEncoder\n\n#Models\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n#model selection\nfrom sklearn.model_selection import GridSearchCV\n\n#evaluation\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-19T07:44:07.40397Z","iopub.execute_input":"2022-06-19T07:44:07.404562Z","iopub.status.idle":"2022-06-19T07:44:08.997122Z","shell.execute_reply.started":"2022-06-19T07:44:07.404471Z","shell.execute_reply":"2022-06-19T07:44:08.996118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv (r'../input/titanic/train.csv')\ntest = pd.read_csv (r'../input/titanic/test.csv')\ntrain.head(15)\n\n# check missing data\ndef missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()/data.isnull().count()*100)\n    types = [str(data[i].dtype) for i in data.columns]\n    df = pd.DataFrame({'Total':total, 'Precent':percent, 'Types':types})\n    return(sp.transpose(df))\n\nmissing_data(train)\nmissing_data(test)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-19T07:44:08.998578Z","iopub.execute_input":"2022-06-19T07:44:08.998844Z","iopub.status.idle":"2022-06-19T07:44:09.067961Z","shell.execute_reply.started":"2022-06-19T07:44:08.998822Z","shell.execute_reply":"2022-06-19T07:44:09.067103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Describe dataset** \n* 11 features contains categorical and continuous and ordinal features.\n* Passengerid may be irralevant\n* need extract information from categorical features like \"Name\",\"Ticket\",\"Cabin\"\n* categorical & ordinal features like \"Embarked\",\"Sex\"need discritization\n\n\n**Handle missing values** \n\nWe notice \"Age\",\"Fare\" ,\"Embarked\", and \"Cabin\" has missing values, we first need to handle the missing values.\n\n*  \"Age\" and \"Fare\"(continuous)\n\nWe construct densiity plot to see its distribution.The ditribution of \"Fare\" is right skewed, hence we use median imputation.\n\n* \"Cabin\" and \"Embarked\" (categorical)\n\nSimply impute \"no values\" to resolve \"Cabin\" missing values. For \"Embarked\", we use the most frequent value imputation.","metadata":{}},{"cell_type":"code","source":"#missing value imputation\ny_train = train['Survived']\nX_train = train.drop(columns='Survived')\ndata = pd.concat([X_train, test])\ndata['Age'].fillna(data['Age'].median(), inplace=True)\ndata['Fare'].fillna(data['Fare'].median(), inplace=True)\ndata['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\ndata['Cabin'].fillna('no value', inplace=True)\ndata['Sex'] = data['Sex'].apply(lambda x: 1 if x == 'male' else 0)\ndata[\"FamilySize\"] = data[\"SibSp\"] + data[\"Parch\"] + 1\n\nregex = \"([A-Za-z]+)\\.\"\ndef get_title(row):\n    match = re.search(regex, str(row))\n    title = match.group(0);\n    return title\ndata['Title'] = data.Name.apply(lambda x: get_title(x))\ndata['Title'] .value_counts()\n","metadata":{"execution":{"iopub.status.busy":"2022-06-19T07:44:09.06919Z","iopub.execute_input":"2022-06-19T07:44:09.069564Z","iopub.status.idle":"2022-06-19T07:44:09.110745Z","shell.execute_reply.started":"2022-06-19T07:44:09.069539Z","shell.execute_reply":"2022-06-19T07:44:09.110079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#https://www.kaggle.com/code/shuvojitdas/predictive-data-analysis-on-titanic-dataset?kernelSessionId=98672795\ndata['Title'] = data['Title'].replace('Mlle.','Miss.')\ndata['Title'] = data['Title'].replace('Ms.','Miss.')  \ndata['Title'] = data['Title'].replace('Mme.','Mrs.')\ndata['Title'] = data['Title'].replace(['Capt.','Col.','Major.'],'Army.')\ndata['Title'] = data['Title'].replace(['Countess.','Don.','Jonkheer.','Lady.','Sir.'],'Noble.')\ndata = data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp', 'Parch','Embarked'], axis = 1)# drop unwanted columns that do not contribute to survivability\n\n#split the train test dataset for traindataset analysis\nX_train = data.loc[:X_train.index[-1]]\nX_test = data.loc[X_train.index[-1]:][1:]\ntrain = pd.concat([y_train, X_train], axis=1)\nX_train","metadata":{"execution":{"iopub.status.busy":"2022-06-19T07:44:09.112391Z","iopub.execute_input":"2022-06-19T07:44:09.113256Z","iopub.status.idle":"2022-06-19T07:44:09.144497Z","shell.execute_reply.started":"2022-06-19T07:44:09.113202Z","shell.execute_reply":"2022-06-19T07:44:09.143545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Anlysis the train dataset**\n\n* seems like sex, pclass and fare is more correlated with survival\n* Embark doesn't seems very helpful\n* children seems have higher survivality -> need to add a binary feature \n* people with titles(noble) ","metadata":{}},{"cell_type":"code","source":"#https://www.kaggle.com/code/shuvojitdas/predictive-data-analysis-on-titanic-dataset?kernelSessionId=98672795\nprint('Total number of passangers who survivded : ', len(train[train['Survived'] == 1]))\nprint('Total number of passangers who died : ', len(train[train['Survived'] == 0]))\n\nprint('Total percentage of male passangers who survivded : ', 100*np.mean(train['Survived'][train['Sex'] == 1]))\nprint('Total percentage of female passangers who survivded : ', 100*np.mean(train['Survived'][train['Sex'] == 0]))\n\nprint('Total percentage of passangers who survivded from first class : ', 100*np.mean(train['Survived'][train['Pclass'] == 1]))\nprint('Total percentage of passangers who survivded from second class : ', 100*np.mean(train['Survived'][train['Pclass'] == 2]))\nprint('Total percentage of passangers who survivded from third class : ', 100*np.mean(train['Survived'][train['Pclass'] == 3]))\n\nprint('Percentage of average survival:\\n\\n{}\\n'.format(train.groupby('Title')['Survived'].mean()*100))\n#print('Percentage of average survival:\\n\\n{}\\n'.format(train.groupby('Embarked')['Survived'].mean()*100))\nprint('Percentage of average survival:\\n\\n{}\\n'.format(train.groupby('Title')['Survived'].mean()*100))\n\nfor i in train:\n    plt.figure(figsize=(13,7))\n    sns.histplot(data = train, x=i, kde=True, hue = 'Survived', multiple='stack')\n    plt.title(i)\n    plt.show()\nplt.figure(figsize = (10, 8))\nsns.heatmap(train.corr(), annot = True)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T07:44:09.145632Z","iopub.execute_input":"2022-06-19T07:44:09.145926Z","iopub.status.idle":"2022-06-19T07:44:12.730036Z","shell.execute_reply.started":"2022-06-19T07:44:09.145878Z","shell.execute_reply":"2022-06-19T07:44:12.728947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature engineering**","metadata":{}},{"cell_type":"code","source":"# add a new column indicating adult or not\ndef children(df): \n    children = [];\n    for i in range(len(df['Age'])):\n        X = df['Age'].iloc[i];\n        if(X>=18):\n            children.append(0);\n        else:\n            children.append(1);\n    df['children'] = children;\n    return df\nchildren(data)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T07:44:12.731624Z","iopub.execute_input":"2022-06-19T07:44:12.732149Z","iopub.status.idle":"2022-06-19T07:44:12.776966Z","shell.execute_reply.started":"2022-06-19T07:44:12.732107Z","shell.execute_reply":"2022-06-19T07:44:12.776268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#discretization\nle = LabelEncoder()\ndata[\"Title\"] = le.fit_transform(data[\"Title\"])\ndata","metadata":{"execution":{"iopub.status.busy":"2022-06-19T07:44:12.778094Z","iopub.execute_input":"2022-06-19T07:44:12.778513Z","iopub.status.idle":"2022-06-19T07:44:12.79687Z","shell.execute_reply.started":"2022-06-19T07:44:12.778476Z","shell.execute_reply":"2022-06-19T07:44:12.796201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#scaler \nsc = StandardScaler()\ntrain_len = X_train.index[-1]\nX_train = data.loc[:train_len]\nsc =sc.fit(X_train[X_train.columns], y_train)\ndata[data.columns] = sc.transform(data[data.columns])\n\nX_train = data.loc[:train_len]\nX_test = data.loc[train_len:][1:]\ntrain = pd.concat([y_train, X_train], axis=1)\ntrain","metadata":{"execution":{"iopub.status.busy":"2022-06-19T07:44:12.798157Z","iopub.execute_input":"2022-06-19T07:44:12.798648Z","iopub.status.idle":"2022-06-19T07:44:12.828699Z","shell.execute_reply.started":"2022-06-19T07:44:12.798612Z","shell.execute_reply":"2022-06-19T07:44:12.827947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.28, random_state = 42)\n\n#scaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T07:44:12.829821Z","iopub.execute_input":"2022-06-19T07:44:12.830168Z","iopub.status.idle":"2022-06-19T07:44:12.844764Z","shell.execute_reply.started":"2022-06-19T07:44:12.830139Z","shell.execute_reply":"2022-06-19T07:44:12.844025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#RF mdoel\nRFmodel = RandomForestClassifier(n_estimators=100, max_depth=len(train.columns)-1, random_state=1)\nRFmodel.fit(X_train, y_train);\nskf = StratifiedKFold(n_splits=10)\nresults = cross_val_score(RFmodel, X_val, y_val, cv=skf)\n\nprint(\"train accuracy: \\n \", metrics.accuracy_score(y_train, RFmodel.predict(X_train)))\nprint(\"cross validation accuracy:  \\n\",results)\nprint(\"Avg cross validation accuracy:  \\n\",np.mean(results))\nplot_confusion_matrix(RFmodel, X_val, y_val)  ","metadata":{"execution":{"iopub.status.busy":"2022-06-19T07:44:12.847004Z","iopub.execute_input":"2022-06-19T07:44:12.847651Z","iopub.status.idle":"2022-06-19T07:44:15.187883Z","shell.execute_reply.started":"2022-06-19T07:44:12.84762Z","shell.execute_reply":"2022-06-19T07:44:15.187186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#logistic regression\nfrom sklearn import svm\nLRmodel= LogisticRegression()\nLRmodel.fit(X_train, y_train)\nskf = StratifiedKFold(n_splits=10)\nresults = cross_val_score(LRmodel, X_val, y_val, cv=skf)\nprint(\"train accuracy: \\n \", metrics.accuracy_score(y_train, LRmodel.predict(X_train)))\nprint(\"cross validation accuracy:  \\n\",results)\nprint(\"Avg cross validation accuracy:  \\n\",np.mean(results))\nplot_confusion_matrix(LRmodel, X_val, y_val) \n","metadata":{"execution":{"iopub.status.busy":"2022-06-19T07:44:15.189133Z","iopub.execute_input":"2022-06-19T07:44:15.189575Z","iopub.status.idle":"2022-06-19T07:44:15.461762Z","shell.execute_reply.started":"2022-06-19T07:44:15.189541Z","shell.execute_reply":"2022-06-19T07:44:15.460958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gradient Boosting\ngadient_boosting = GradientBoostingClassifier()\ngadient_boosting.fit(X_train, y_train)\nskf = StratifiedKFold(n_splits=10)\nresults = cross_val_score(gadient_boosting, X_val, y_val, cv=skf)\n\nprint(\"train accuracy: \\n \", metrics.accuracy_score(y_train, gadient_boosting.predict(X_train)))\nprint(\"cross validation accuracy:  \\n\",results)\nprint(\"Avg cross validation accuracy:  \\n\",np.mean(results))\nplot_confusion_matrix(gadient_boosting, X_val, y_val)  \n","metadata":{"execution":{"iopub.status.busy":"2022-06-19T07:44:15.463254Z","iopub.execute_input":"2022-06-19T07:44:15.463633Z","iopub.status.idle":"2022-06-19T07:44:16.488939Z","shell.execute_reply.started":"2022-06-19T07:44:15.463596Z","shell.execute_reply":"2022-06-19T07:44:16.487885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#maybe not to use stacking since no improve in result\n'''from sklearn.ensemble import StackingClassifier\nestimators = [('rf',RFmodel), ('lr', LRmodel),('gradient_boosting',gadient_boosting)]\nclf = StackingClassifier(estimators=estimators, final_estimator=RFmodel)\nclf.fit(X_train, y_train).score(X_val, y_val)'''","metadata":{"execution":{"iopub.status.busy":"2022-06-19T07:44:16.490164Z","iopub.execute_input":"2022-06-19T07:44:16.490564Z","iopub.status.idle":"2022-06-19T07:44:16.496508Z","shell.execute_reply.started":"2022-06-19T07:44:16.490532Z","shell.execute_reply":"2022-06-19T07:44:16.495787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = RFmodel.predict(X_test)\noutput = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': y_test})\noutput.head()\noutput.to_csv('./submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T07:44:16.497528Z","iopub.execute_input":"2022-06-19T07:44:16.498Z","iopub.status.idle":"2022-06-19T07:44:16.538962Z","shell.execute_reply.started":"2022-06-19T07:44:16.49797Z","shell.execute_reply":"2022-06-19T07:44:16.538187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}